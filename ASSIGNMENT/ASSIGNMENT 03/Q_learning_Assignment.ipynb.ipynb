{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "q1rNHXSVtFjd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import json\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "\n",
        "with open(\"themes.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    THEMES = json.load(f)\n",
        "\n",
        "# ========== CONFIG ==========\n",
        "maze_size = (20, 20)\n",
        "participant_id = 23801 ## SR.No\n",
        "enable_enemy = True\n",
        "enable_trap_boost = True\n",
        "save_path = f\"{participant_id}.pkl\"\n",
        "\n",
        "# Q-learning parameters\n",
        "###################################\n",
        "#      WRITE YOUR CODE BELOW      #\n",
        "num_actions = 4\n",
        "gamma = 0.99           # between 0 - 1\n",
        "alpha = 0.8            # between 0 - 1\n",
        "epsilon = 1.0          # between 0 - 1\n",
        "epsilon_decay = 0.9995  # between 0.1 - 1\n",
        "min_epsilon = 0.1\n",
        "num_episodes = 10000   # Total number of episodes\n",
        "max_steps = 300        # Maximum steps per episode\n",
        "###################################     \n",
        "\n",
        "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # Up, Down, Left, Right\n",
        "\n",
        "# ========== REWARDS ==========\n",
        "###################################\n",
        "#      WRITE YOUR CODE BELOW      #\n",
        "REWARD_GOAL     = 100  # Reward for reaching goal.\n",
        "REWARD_TRAP     = -1000  # Trap cell.\n",
        "REWARD_OBSTACLE = -1000  # Obstacle cell.\n",
        "REWARD_REVISIT  = -10   # Revisiting same cell.\n",
        "REWARD_ENEMY    = -2000 # Getting caught by enemy.\n",
        "REWARD_STEP     = -1   # Per-step time penalty.\n",
        "REWARD_BOOST    = 20   # Boost cell.\n",
        "###################################\n",
        "# =============================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "FKJ5KM_rtJ3q"
      },
      "outputs": [],
      "source": [
        "# Environment\n",
        "class MazeGymEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, maze_size, participant_id, enable_enemy, enable_trap_boost, max_steps):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        initialize the maze_size, participant_id, enable_enemy, enable_trap_boost and max_steps variables\n",
        "        \"\"\"\n",
        "        ###################################\n",
        "        #      WRITE YOUR CODE BELOW      #\n",
        "        self.maze_size = maze_size\n",
        "        self.participant_id = participant_id\n",
        "        self.enable_enemy = enable_enemy\n",
        "        self.enable_trap_boost = enable_trap_boost\n",
        "        self.max_steps = max_steps\n",
        "        ###################################\n",
        "\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "        self.observation_space = spaces.Tuple((\n",
        "            spaces.Discrete(maze_size[0]),\n",
        "            spaces.Discrete(maze_size[1])\n",
        "        ))\n",
        "\n",
        "        \"\"\"\n",
        "        generate  self.maze using the _generate_obstacles method\n",
        "        make self.start as the top left cell of the maze and self.goal as the bottom right\n",
        "        \"\"\"\n",
        "        ###################################\n",
        "        #      WRITE YOUR CODE BELOW      #\n",
        "        self.maze = self._generate_obstacles()\n",
        "        self.start = (0, 0)\n",
        "        self.goal = (self.maze_size[0]-1, self.maze_size[1]-1)\n",
        "        self.visited = set()\n",
        "        ###################################\n",
        "\n",
        "        if self.enable_trap_boost:\n",
        "            self.trap_cells, self.boost_cells = self._generate_traps_and_boosts(self.maze)\n",
        "        else:\n",
        "            self.trap_cells, self.boost_cells = ([], [])\n",
        "\n",
        "        self.enemy_cells = []\n",
        "        self.current_step = 0\n",
        "        self.agent_pos = None\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def _generate_obstacles(self):\n",
        "        \"\"\"\n",
        "        generates the maze with random obstacles based on the SR.No.\n",
        "        \"\"\"\n",
        "        np.random.seed(self.participant_id)\n",
        "        maze = np.zeros(self.maze_size, dtype=int)\n",
        "        mask = np.ones(self.maze_size, dtype=bool)\n",
        "        safe_cells = [\n",
        "            (0, 0), (0, 1), (1, 0),\n",
        "            (self.maze_size[0]-1, self.maze_size[1]-1), (self.maze_size[0]-2, self.maze_size[1]-1),\n",
        "            (self.maze_size[0]-1, self.maze_size[1]-2)\n",
        "        ]\n",
        "        for row, col in safe_cells:\n",
        "            mask[row, col] = False\n",
        "        maze[mask] = np.random.choice([0, 1], size=mask.sum(), p=[0.9, 0.1])\n",
        "        return maze\n",
        "\n",
        "    def _generate_traps_and_boosts(self, maze):\n",
        "        \"\"\"\n",
        "        generates special cells, traps and boosts. While training our agent,\n",
        "        we want to pass thru more number of boost cells and avoid trap cells \n",
        "        \"\"\"\n",
        "        if not self.enable_trap_boost:\n",
        "            return [], []\n",
        "        exclusions = {self.start, self.goal}\n",
        "        empty_cells = list(zip(*np.where(maze == 0)))\n",
        "        valid_cells = [cell for cell in empty_cells if cell not in exclusions]\n",
        "        num_traps = self.maze_size[0] * 2\n",
        "        num_boosts = self.maze_size[0] * 2\n",
        "        random.seed(self.participant_id)\n",
        "        trap_cells = random.sample(valid_cells, num_traps)\n",
        "        trap_cells_ = trap_cells\n",
        "        remaining_cells = [cell for cell in valid_cells if cell not in trap_cells]\n",
        "        boost_cells = random.sample(remaining_cells, num_boosts)\n",
        "        boost_cells_ = boost_cells\n",
        "        return trap_cells, boost_cells\n",
        "\n",
        "    def move_enemy(self, enemy_pos):\n",
        "        possible_moves = []\n",
        "        for dx, dy in actions:\n",
        "            new_pos = (enemy_pos[0] + dx, enemy_pos[1] + dy)\n",
        "            if (0 <= new_pos[0] < self.maze_size[0] and\n",
        "                0 <= new_pos[1] < self.maze_size[1] and\n",
        "                self.maze[new_pos] != 1):\n",
        "                possible_moves.append(new_pos)\n",
        "        return random.choice(possible_moves) if possible_moves else enemy_pos\n",
        "\n",
        "    def update_enemies(self):\n",
        "        if self.enable_enemy:\n",
        "            self.enemy_cells = [self.move_enemy(enemy) for enemy in self.enemy_cells]\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        if seed is not None:\n",
        "            random.seed(seed)\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        empty_cells = list(zip(*np.where(self.maze == 0)))\n",
        "        self.start = (0, 0)\n",
        "        self.goal = (self.maze_size[0]-1, self.maze_size[1]-1)\n",
        "\n",
        "        for pos in (self.start, self.goal):\n",
        "            if pos in self.trap_cells:\n",
        "                self.trap_cells.remove(pos)\n",
        "            if pos in self.boost_cells:\n",
        "                self.boost_cells.remove(pos)\n",
        "\n",
        "        if self.enable_enemy:\n",
        "            enemy_candidates = [cell for cell in empty_cells if cell not in {self.start, self.goal}]\n",
        "            num_enemies = max(1, int((self.maze_size[0] * self.maze_size[1]) / 100))\n",
        "            self.enemy_cells = random.sample(enemy_candidates, min(num_enemies, len(enemy_candidates)))\n",
        "        else:\n",
        "            self.enemy_cells = []\n",
        "\n",
        "        self.current_step = 0\n",
        "        self.agent_pos = self.start\n",
        "        self.visited = set()\n",
        "\n",
        "\n",
        "        return self.agent_pos, {}\n",
        "\n",
        "    def get_reward(self, state):\n",
        "        if state == self.goal:\n",
        "            return REWARD_GOAL\n",
        "        elif state in self.trap_cells:\n",
        "            return REWARD_TRAP\n",
        "        elif state in self.boost_cells:\n",
        "            return REWARD_BOOST\n",
        "        elif self.maze[state] == 1:\n",
        "            return REWARD_OBSTACLE\n",
        "        else:\n",
        "            return REWARD_STEP\n",
        "\n",
        "    def take_action(self, state, action):\n",
        "        attempted_state = (state[0] + actions[action][0], state[1] + actions[action][1])\n",
        "        if (0 <= attempted_state[0] < self.maze_size[0] and\n",
        "            0 <= attempted_state[1] < self.maze_size[1] and\n",
        "            self.maze[attempted_state] != 1):\n",
        "            return attempted_state, False\n",
        "        else:\n",
        "            return state, True\n",
        "\n",
        "    def step(self, action):\n",
        "        self.current_step += 1\n",
        "        next_state, wall_collision = self.take_action(self.agent_pos, action)\n",
        "        if wall_collision:\n",
        "            reward = REWARD_OBSTACLE\n",
        "            next_state = self.agent_pos\n",
        "        else:\n",
        "            if self.enable_enemy:\n",
        "                self.update_enemies()\n",
        "            if self.enable_enemy and next_state in self.enemy_cells:\n",
        "                reward = REWARD_ENEMY\n",
        "                done = True\n",
        "                truncated = True\n",
        "                info = {'terminated_by': 'enemy'}\n",
        "                self.agent_pos = next_state\n",
        "                return self.agent_pos, reward, done, truncated, info\n",
        "            else:\n",
        "                revisit_penalty = REWARD_REVISIT if next_state in self.visited else 0\n",
        "                self.visited.add(next_state)\n",
        "                reward = self.get_reward(next_state) + revisit_penalty\n",
        "        self.agent_pos = next_state\n",
        "\n",
        "        if self.agent_pos == self.goal:\n",
        "            done = True\n",
        "            truncated = False\n",
        "            info = {'completed_by': 'goal'}\n",
        "        elif self.current_step >= self.max_steps:\n",
        "            done = True\n",
        "            truncated = True\n",
        "            info = {'terminated_by': 'timeout'}\n",
        "        else:\n",
        "            done = False\n",
        "            truncated = False\n",
        "            info = {\n",
        "                'current_step': self.current_step,\n",
        "                'agent_position': self.agent_pos,\n",
        "                'remaining_steps': self.max_steps - self.current_step\n",
        "            }\n",
        "\n",
        "        return self.agent_pos, reward, done, truncated, info\n",
        "\n",
        "    def render(self, path=None, theme=\"racing\"):\n",
        "        icons = THEMES.get(theme, THEMES[\"racing\"])\n",
        "        clear_output(wait=True)\n",
        "        grid = np.full(self.maze_size, icons[\"empty\"])\n",
        "        grid[self.maze == 1] = icons[\"obstacle\"]\n",
        "        for cell in self.trap_cells:\n",
        "            grid[cell] = icons[\"trap\"]\n",
        "        for cell in self.boost_cells:\n",
        "            grid[cell] = icons[\"boost\"]\n",
        "        grid[self.start] = icons[\"start\"]\n",
        "        grid[self.goal] = icons[\"goal\"]\n",
        "        if path is not None:\n",
        "            for cell in path[1:-1]:\n",
        "                if grid[cell] not in (icons[\"goal\"], icons[\"obstacle\"], icons[\"trap\"], icons[\"boost\"]):\n",
        "                    grid[cell] = icons[\"path\"]\n",
        "        if self.agent_pos is not None:\n",
        "            if grid[self.agent_pos] not in (icons[\"goal\"], icons[\"obstacle\"]):\n",
        "                grid[self.agent_pos] = icons[\"agent\"]\n",
        "        if self.enable_enemy:\n",
        "            for enemy in self.enemy_cells:\n",
        "                grid[enemy] = icons[\"enemy\"]\n",
        "        df = pd.DataFrame(grid)\n",
        "        print(df.to_string(index=False, header=False))\n",
        "\n",
        "    def print_final_message(self, success, interrupted, caught, theme):\n",
        "        msgs = THEMES.get(theme, THEMES[\"racing\"]).get(\"final_messages\", {})\n",
        "        if interrupted:\n",
        "            print(f\"\\n{msgs.get('Interrupted', 'ğŸ›‘ Interrupted.')}\")\n",
        "        elif caught:\n",
        "            print(f\"\\n{msgs.get('Defeat', 'ğŸš“ Caught by enemy.')}\")\n",
        "        elif success:\n",
        "            print(f\"\\n{msgs.get('Triumph', 'ğŸ Success.')}\")\n",
        "        else:\n",
        "            print(f\"\\n{msgs.get('TimeOut', 'â›½ Time Out.')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-En22A4ftMdT"
      },
      "outputs": [],
      "source": [
        "# Agent\n",
        "class QLearningAgent:\n",
        "    def __init__(self, maze_size, num_actions, alpha=0.1, gamma=0.99):\n",
        "        \"\"\"\n",
        "        initialize self.num_actions, self.alpha, self.gamma\n",
        "        initialize self.q_table based on number of states and number of actions\n",
        "        \"\"\"\n",
        "        ###################################\n",
        "        #      WRITE YOUR CODE BELOW      #\n",
        "        self.num_actions = num_actions\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.q_table = np.zeros((maze_size[0], maze_size[1], num_actions))\n",
        "        ###################################\n",
        "        \n",
        "\n",
        "    def choose_action(self, env, state, epsilon):\n",
        "        \"\"\"\n",
        "        returns an integer between [0,3]\n",
        "\n",
        "        epsilon is a parameter between 0 and 1.\n",
        "        It is the probability with which we choose an exploratory action (random action)\n",
        "        Eg: ---\n",
        "        If epsilon = 0.25, probability of choosing action from q_table = 0.75\n",
        "                           probability of choosing random action = 0.25\n",
        "        \"\"\"\n",
        "        ###################################\n",
        "        #      WRITE YOUR CODE BELOW      #\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            return random.randint(0, self.num_actions - 1)\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state[0], state[1]])\n",
        "        ###################################\n",
        "\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        \"\"\"\n",
        "        Use the Q-learning update equation to update the Q-Table\n",
        "        \"\"\"\n",
        "        ###################################\n",
        "        #      WRITE YOUR CODE BELOW      #\n",
        "        best_next_action = np.argmax(self.q_table[next_state[0], next_state[1]])\n",
        "        td_target = reward + self.gamma * self.q_table[next_state[0], next_state[1], best_next_action]\n",
        "        td_error = td_target - self.q_table[state[0], state[1], action]\n",
        "        self.q_table[state[0], state[1], action] += self.alpha * td_error\n",
        "        ###################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ebdQy-8tPw-",
        "outputId": "ac14a863-71c6-47af-f00e-842c43502cc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New best at episode 0: 107 steps and Reward -27131.00\n",
            "Episode 0/10000 - Epsilon: 0.9474 - Total Steps: 107 - Episode Reward: -27131.00 - Best Reward: -27131.00\n",
            "New best at episode 2: 119 steps and Reward -23303.00\n",
            "New best at episode 7: 17 steps and Reward -3008.00\n",
            "New best at episode 20: 37 steps and Reward -2168.00\n",
            "New best at episode 28: 21 steps and Reward -2055.00\n",
            "New best at episode 40: 10 steps and Reward -1995.00\n",
            "New best at episode 61: 13 steps and Reward -1948.00\n",
            "New best at episode 117: 142 steps and Reward -1813.00\n",
            "New best at episode 167: 62 steps and Reward 66.00\n",
            "New best at episode 709: 52 steps and Reward 137.00\n",
            "Episode 1000/10000 - Epsilon: 0.1000 - Total Steps: 225 - Episode Reward: -10299.00 - Best Reward: 137.00\n",
            "Episode 2000/10000 - Epsilon: 0.1000 - Total Steps: 59 - Episode Reward: -3581.00 - Best Reward: 137.00\n",
            "New best at episode 2465: 60 steps and Reward 189.00\n",
            "Episode 3000/10000 - Epsilon: 0.1000 - Total Steps: 89 - Episode Reward: -1264.00 - Best Reward: 189.00\n",
            "Episode 4000/10000 - Epsilon: 0.1000 - Total Steps: 229 - Episode Reward: -16403.00 - Best Reward: 189.00\n",
            "New best at episode 4223: 60 steps and Reward 231.00\n",
            "Episode 5000/10000 - Epsilon: 0.1000 - Total Steps: 148 - Episode Reward: -3259.00 - Best Reward: 231.00\n",
            "Episode 6000/10000 - Epsilon: 0.1000 - Total Steps: 48 - Episode Reward: 59.00 - Best Reward: 231.00\n",
            "Episode 7000/10000 - Epsilon: 0.1000 - Total Steps: 300 - Episode Reward: -11538.00 - Best Reward: 231.00\n",
            "Episode 8000/10000 - Epsilon: 0.1000 - Total Steps: 31 - Episode Reward: -2094.00 - Best Reward: 231.00\n",
            "Episode 9000/10000 - Epsilon: 0.1000 - Total Steps: 86 - Episode Reward: -2418.00 - Best Reward: 231.00\n",
            "\n",
            "Training completed. Total episodes: 9999\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "env = MazeGymEnv(maze_size, participant_id, enable_enemy, enable_trap_boost, max_steps)\n",
        "agent = QLearningAgent(maze_size, num_actions)\n",
        "\n",
        "start_episode = 0\n",
        "best_reward = -np.inf\n",
        "best_q_table = None\n",
        "\n",
        "if os.path.exists(save_path):\n",
        "    print(\"Checkpoint found. Loading...\")\n",
        "    with open(save_path, 'rb') as f:\n",
        "        checkpoint = pickle.load(f)\n",
        "        agent.q_table = checkpoint['q_table']\n",
        "        start_episode = checkpoint['episode']\n",
        "        epsilon = checkpoint['epsilon']\n",
        "        best_q_table = checkpoint.get('best_q_table', agent.q_table.copy())\n",
        "        best_reward = checkpoint.get('best_reward', -np.inf)\n",
        "        best_step_counter = checkpoint.get('best_step_counter', 0)\n",
        "    print(f\"Resuming from episode {start_episode} with epsilon {epsilon:.4f}, best reward {best_reward:.2f} and best step {best_step_counter}\")\n",
        "else:\n",
        "    epsilon = 1.0\n",
        "\n",
        "try:\n",
        "    for episode in range(start_episode, num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        visited_states = set()\n",
        "        episode_reward = 0\n",
        "        step_counter = 0\n",
        "\n",
        "        while not done and step_counter < max_steps:\n",
        "            action = agent.choose_action(env, state, epsilon)\n",
        "            epsilon = max(min_epsilon, epsilon * epsilon_decay) # mene add kia\n",
        "            next_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "            if next_state in visited_states:\n",
        "                reward += REWARD_REVISIT\n",
        "            visited_states.add(next_state)\n",
        "\n",
        "            agent.update(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            step_counter += 1\n",
        "\n",
        "            if state == env.goal:\n",
        "                done = True\n",
        "\n",
        "        if episode_reward > best_reward:\n",
        "            best_reward = episode_reward\n",
        "            best_q_table = agent.q_table.copy()\n",
        "            best_step_counter = step_counter\n",
        "            with open(save_path, 'wb') as f:\n",
        "                pickle.dump({\n",
        "                    'q_table': agent.q_table,\n",
        "                    'episode': episode,\n",
        "                    'epsilon': epsilon,\n",
        "                    'best_q_table': best_q_table,\n",
        "                    'best_reward': best_reward,\n",
        "                    'best_step_counter': best_step_counter\n",
        "                }, f)\n",
        "            print(f\"New best at episode {episode}: {step_counter} steps and Reward {best_reward:.2f}\")\n",
        "\n",
        "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "        if episode % 1000 == 0:\n",
        "            print(f\"Episode {episode}/{num_episodes} - Epsilon: {epsilon:.4f} - Total Steps: {step_counter} - Episode Reward: {episode_reward:.2f} - Best Reward: {best_reward:.2f}\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nTraining interrupted.\")\n",
        "    print(f\"Interrupted at episode {episode} with epsilon: {epsilon:.4f}, Total Steps: {step_counter}, Episode Reward: {episode_reward:.2f}\")\n",
        "else:\n",
        "    print(f\"\\nTraining completed. Total episodes: {episode}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "bVahczp5tZTY"
      },
      "outputs": [],
      "source": [
        "def test_agent(env, agent, animated, delay, theme):\n",
        "\n",
        "    obs, _ = env.reset()\n",
        "    state = obs\n",
        "    path = [state]\n",
        "    visited_states = set()\n",
        "    total_reward = 0\n",
        "    reward_breakdown = {\n",
        "        'goal':     {'count': 0, 'reward': 0.0},\n",
        "        'trap':     {'count': 0, 'reward': 0.0},\n",
        "        'boost':    {'count': 0, 'reward': 0.0},\n",
        "        'obstacle': {'count': 0, 'reward': 0.0},\n",
        "        'step':     {'count': 0, 'reward': 0.0},\n",
        "        'revisit':  {'count': 0, 'reward': 0.0}\n",
        "    }\n",
        "    caught_by_enemy = False\n",
        "    success = False\n",
        "    interrupted = False\n",
        "\n",
        "    try:\n",
        "        for step in range(env.max_steps):\n",
        "            visited_states.add(state)\n",
        "\n",
        "            action = agent.choose_action(env, state, epsilon=0.0)\n",
        "            next_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "            if info.get('terminated_by') == 'enemy':\n",
        "                caught_by_enemy = True\n",
        "                reward_breakdown.setdefault('enemy', {'count': 0, 'reward': 0.0})\n",
        "                reward_breakdown['enemy']['count'] += 1\n",
        "                reward_breakdown['enemy']['reward'] += reward\n",
        "                total_reward += reward\n",
        "                path.append(next_state)\n",
        "                break\n",
        "            else:\n",
        "                if (next_state == state) and (reward == REWARD_OBSTACLE):\n",
        "                    reward_breakdown['obstacle']['count'] += 1\n",
        "                    reward_breakdown['obstacle']['reward'] += REWARD_OBSTACLE\n",
        "                elif next_state == env.goal:\n",
        "                    reward_breakdown['goal']['count'] += 1\n",
        "                    reward_breakdown['goal']['reward'] += REWARD_GOAL\n",
        "                elif next_state in env.trap_cells:\n",
        "                    reward_breakdown['trap']['count'] += 1\n",
        "                    reward_breakdown['trap']['reward'] += REWARD_TRAP\n",
        "                elif next_state in env.boost_cells:\n",
        "                    reward_breakdown['boost']['count'] += 1\n",
        "                    reward_breakdown['boost']['reward'] += REWARD_BOOST\n",
        "                elif next_state in visited_states:\n",
        "                    reward += REWARD_REVISIT\n",
        "                    reward_breakdown['revisit']['count'] += 1\n",
        "                    reward_breakdown['revisit']['reward'] += REWARD_REVISIT\n",
        "                reward_breakdown['step']['count'] += 1\n",
        "                reward_breakdown['step']['reward'] += REWARD_STEP\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "            path.append(state)\n",
        "\n",
        "            if animated:\n",
        "                env.render(path, theme)\n",
        "                print(f\"\\nTotal Allowed Ateps: {env.max_steps}\")\n",
        "                print(f\"Current Reward: {total_reward:.2f}\")\n",
        "                print(\"Live Reward Breakdown:\")\n",
        "                df = pd.DataFrame.from_dict(reward_breakdown, orient='index')\n",
        "                print(df)\n",
        "                time.sleep(delay)\n",
        "\n",
        "            if done or truncated:\n",
        "                break\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        interrupted = True\n",
        "\n",
        "    if state == env.goal:\n",
        "        success = True\n",
        "\n",
        "    return path, total_reward, reward_breakdown, success, interrupted, caught_by_enemy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NkA-fGy4EAS",
        "outputId": "a0658c4e-39a0-4fe6-ac6c-f449d295ea6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint found. Loading best Q-table for testing...\n",
            "Best Q-table loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "env = MazeGymEnv(maze_size, participant_id, enable_enemy, enable_trap_boost, max_steps)\n",
        "agent = QLearningAgent(maze_size, num_actions)\n",
        "\n",
        "if os.path.exists(save_path):\n",
        "    print(\"Checkpoint found. Loading best Q-table for testing...\")\n",
        "    with open(save_path, 'rb') as f:\n",
        "        checkpoint = pickle.load(f)\n",
        "        best_q_table = checkpoint.get('best_q_table', checkpoint['q_table'])\n",
        "        agent.q_table = best_q_table\n",
        "    print(\"Best Q-table loaded successfully.\")\n",
        "else:\n",
        "    print(\"No checkpoint found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqEw69hYsmtA",
        "outputId": "af0971ad-cfba-496e-de79-fb3e10fc1051"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš¢ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸ¦ˆ ğŸŒ€ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸ  ğŸŸ¦ ğŸŸ¦ ğŸŒ€ ğŸŸ¦ ğŸŸ¦\n",
            "ğŸ  ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸ  ğŸŸ¦ ğŸ  ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸ¦ˆ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦\n",
            "ğŸŒŠ ğŸŸ¦ ğŸŸ¦ ğŸŒ€ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŒ€ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸ¦ˆ\n",
            "ğŸŒŠ ğŸ¦ˆ ğŸ¦ˆ ğŸ™ ğŸŒ€ ğŸŸ¦ ğŸ¦ˆ ğŸŸ¦ ğŸ¦ˆ ğŸ  ğŸŸ¦ ğŸ¦ˆ ğŸ¦ˆ ğŸ  ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸ¦ˆ\n",
            "ğŸŒŠ ğŸŒŠ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸ  ğŸŸ¦ ğŸŒ€ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸ  ğŸ¦ˆ\n",
            "ğŸŒ€ ğŸŒŠ ğŸŒŠ ğŸ¦ˆ ğŸ  ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŒ€ ğŸ  ğŸŸ¦ ğŸŸ¦ ğŸŒ€ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŒ€ ğŸŸ¦ ğŸ \n",
            "ğŸŸ¦ ğŸŒ€ ğŸ  ğŸŒ€ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸ¦ˆ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŒ€ ğŸ  ğŸŒ€ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦\n",
            "ğŸ  ğŸŸ¦ ğŸŒŠ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŒ€ ğŸŸ¦ ğŸ¦ˆ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸ™\n",
            "ğŸŸ¦ ğŸŸ¦ ğŸŒŠ ğŸŸ¦ ğŸ  ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸ  ğŸŸ¦ ğŸŸ¦ ğŸ  ğŸŸ¦ ğŸ¦ˆ ğŸ  ğŸŒ€ ğŸ  ğŸ  ğŸŒ€\n",
            "ğŸ¦ˆ ğŸŸ¦ ğŸ  ğŸŒŠ ğŸ¦ˆ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸ  ğŸŸ¦ ğŸŸ¦ ğŸŒ€ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸ™ ğŸ¦ˆ ğŸŸ¦\n",
            "ğŸŸ¦ ğŸŒ€ ğŸŸ¦ ğŸŒŠ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸ¦ˆ ğŸŸ¦ ğŸ  ğŸŸ¦ ğŸŸ¦ ğŸŒ€ ğŸŸ¦ ğŸŒ€ ğŸ  ğŸŸ¦ ğŸŸ¦ ğŸŸ¦\n",
            "ğŸŸ¦ ğŸ¦ˆ ğŸ¦ˆ ğŸŒŠ ğŸŸ¦ ğŸŸ¦ ğŸ¦ˆ ğŸ  ğŸ¦ˆ ğŸŸ¦ ğŸŸ¦ ğŸŒ€ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸ  ğŸ¦ˆ ğŸŸ¦ ğŸŸ¦ ğŸŒ€\n",
            "ğŸŸ¦ ğŸ  ğŸŒŠ ğŸŒŠ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŒ€ ğŸ  ğŸ¦ˆ ğŸŒ€ ğŸŸ¦ ğŸŸ¦ ğŸ¦ˆ ğŸ™ ğŸŸ¦ ğŸ  ğŸ  ğŸ¦ˆ ğŸŸ¦\n",
            "ğŸŸ¦ ğŸŒŠ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŒ€ ğŸŸ¦ ğŸ  ğŸŸ¦ ğŸŒ€ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸ¦ˆ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦\n",
            "ğŸŸ¦ ğŸ  ğŸŸ¦ ğŸ¦ˆ ğŸŒ€ ğŸŸ¦ ğŸŸ¦ ğŸŒ€ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŒŠ ğŸŒŠ ğŸŸ¦ ğŸŸ¦ ğŸ¦ˆ ğŸŸ¦ ğŸ¦ˆ\n",
            "ğŸŒŠ ğŸŒŠ ğŸŒ€ ğŸŒŠ ğŸŒŠ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸ¦ˆ ğŸŸ¦ ğŸ¦ˆ ğŸŒŠ ğŸŒŠ ğŸŒŠ ğŸŒŠ ğŸŒŠ ğŸ¦ˆ ğŸŸ¦ ğŸ¦ˆ ğŸŸ¦\n",
            "ğŸŒŠ ğŸŒŠ ğŸ  ğŸŒŠ ğŸŒŠ ğŸŸ¦ ğŸŸ¦ ğŸ¦ˆ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŒŠ ğŸŒŠ ğŸ  ğŸŸ¦ ğŸŒŠ ğŸ  ğŸŒŠ ğŸ  ğŸŒŠ\n",
            "ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŒ€ ğŸŒŠ ğŸŒŠ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŒŠ ğŸ  ğŸŒŠ ğŸŒŠ ğŸŸ¦ ğŸŸ¦ ğŸ¦ˆ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŒŠ\n",
            "ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŒŠ ğŸŒŠ ğŸŒŠ ğŸ  ğŸŒŠ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŒ€ ğŸŸ¦ ğŸŒŠ\n",
            "ğŸ  ğŸŸ¦ ğŸ  ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŒ€ ğŸŸ¦ ğŸ¦ˆ ğŸŸ¦ ğŸ¦ˆ ğŸŸ¦ ğŸŸ¦ ğŸŸ¦ ğŸŒ€ ğŸŸ¦ ğŸŸ¦ ğŸ¦ˆ ğŸŸ¦ ğŸš£\n",
            "\n",
            "ğŸï¸ Aye aye! You discovered the hidden island!\n",
            "         Count  Reward\n",
            "Goal         1   100.0\n",
            "Trap         0     0.0\n",
            "Boost       10   200.0\n",
            "Obstacle     0     0.0\n",
            "Step        56   -56.0\n",
            "Revisit      0     0.0\n",
            "Total            244.0\n",
            "\n",
            "Total Allowed Ateps: 300\n"
          ]
        }
      ],
      "source": [
        "# Run test.\n",
        "\n",
        "theme = random.choice(list(THEMES.keys()))\n",
        "plot_delay = 0.1  # Adjust delay as needed\n",
        "\n",
        "path, total_reward, reward_breakdown, success, interrupted, caught_by_enemy = test_agent(env, agent, animated=True, delay=plot_delay, theme=theme)\n",
        "\n",
        "env.render(path, theme=theme)\n",
        "env.print_final_message(success, interrupted, caught=caught_by_enemy, theme=theme)\n",
        "\n",
        "reward_df = pd.DataFrame.from_dict(reward_breakdown, orient='index')\n",
        "reward_df.index = reward_df.index.str.title()\n",
        "reward_df = reward_df.rename(columns={'count': 'Count', 'reward': 'Reward'})\n",
        "total_row = pd.DataFrame({\n",
        "    'Count': [''],\n",
        "    'Reward': [reward_df['Reward'].sum()]\n",
        "}, index=['Total'])\n",
        "reward_df = pd.concat([reward_df, total_row])\n",
        "\n",
        "print(reward_df)\n",
        "print(f\"\\nTotal Allowed Ateps: {max_steps}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rename and Submit this file as **SRNO(5digit)_Assignment3.ipynb**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "umc203",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
